# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply prepend
# them with $. For strings the variable must be within quotes (ie, "$STR_VAR"),
# for numbers and booleans they should be plain (ie, $INT_VAR, $BOOL_VAR)


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"


# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  ## This buffer only fills when writes fail to output plugin(s).
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. You shouldn't set this below
  ## interval. Maximum flush_interval will be flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Logging configuration:
  ## Run telegraf with debug log messages.
  debug = false
  ## Run telegraf in quiet mode (error log messages only).
  quiet = false
  ## Specify the log file name. The empty string means to log to stderr.
  logfile = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# Configuration for influxdb server to send metrics to
[[outputs.influxdb]]
  ## The HTTP or UDP URL for your InfluxDB instance.  Each item should be
  ## of the form:
  ##   scheme "://" host [ ":" port]
  ##
  ## Multiple urls can be specified as part of the same cluster,
  ## this means that only ONE of the urls will be written to each interval.
  # urls = ["udp://localhost:8089"] # UDP endpoint example
  urls = ["http://${INFLUXDB_HOST}:8086"] # required
  ## The target database for metrics (telegraf will create it if not exists).
  database = "${INFLUXDB_MONITORING_DB}" # required

  ## Name of existing retention policy to write to.  Empty string writes to
  ## the default retention policy.
  retention_policy = "monitoring_raw"
  ## Write consistency (clusters only), can be: "any", "one", "quorum", "all"
  write_consistency = "any"

  ## Write timeout (for the InfluxDB client), formatted as a string.
  ## If not provided, will default to 5s. 0s means no timeout (not recommended).
  timeout = "5s"
  # username = "telegraf"
  # password = "metricsmetricsmetricsmetrics"
  ## Set the user agent for HTTP POSTs (can be useful for log differentiation)
  # user_agent = "telegraf"
  ## Set UDP payload size, defaults to InfluxDB UDP Client default (512 bytes)
  # udp_payload = 512

#[[outputs.file]]
   ## Files to write to, "stdout" is a specially handled file.
#   files = ["stdout"]

# # Read Nginx's basic status information (ngx_http_stub_status_module)
[[inputs.nginx]]
#   ## An array of Nginx stub_status URI to gather stats.
  urls = ["http://${NGINX_HOST}:10080/nginx_status"]


###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################


[[inputs.tail]]
   ## files to tail.
  files = ["/var/log/nginx/access.log"]
  ## Override the default measurement name, which would be "logparser_grok"
  name_override = "nginx_access_log"
  ## For parsing logstash-style "grok" patterns:
  data_format = "grok"
  grok_patterns = ["%{COMBINED_LOG_FORMAT}"]

[[inputs.tail]]
  ## files to tail.
  files = ["/var/log/nginx/error.log"]
  ## Override the default measurement name, which would be "logparser_grok"
  name_override = "nginx_error_log"
  data_format = "grok"
  ## For parsing logstash-style "grok" patterns:
  grok_patterns = ['%{TIMESTAMP:timestamp:ts-"2006/01/02 15:04:05"} %{CUSTOM_LOG}']
  grok_custom_patterns = '''
    TIMESTAMP %{YEAR}[./]%{MONTHNUM}[./]%{MONTHDAY} %{TIME}
    CUSTOM_LOG \[%{LOGLEVEL:severity:tag}\] %{POSINT:pid:drop}#%{NUMBER:threadid:drop}\: \*%{NUMBER:connectionid:drop} %{GREEDYDATA:message}, client: %{IP:client}, server: %{GREEDYDATA:server:drop}, request: "(?:%{WORD:verb} %{NOTSPACE:request:drop}(?: HTTP/%{NUMBER:httpversion:drop}))", host: %{GREEDYDATA:host}
  '''

[[inputs.tail]]
   ## files to tail.
  files = ["/var/log/modules/modules.log", "/var/log/modules/thermostat.log"]
  ## Override the default measurement name, which would be "logparser_grok"
  name_override = "modules_log"
  data_format = "grok"
  ## For parsing logstash-style "grok" patterns:
  grok_patterns = ['%{TIMESTAMP_ISO8601:timestamp:ts-"2006-01-02 15:04:05.000"} \<%{LOGLEVEL:log_level:tag}\> %{FUNCTION_NAME:function_name:tag}:%{POSINT:line_number:int}: %{GREEDYDATA:GREEDYDATA}']
  grok_custom_patterns = '''
    LOGLEVEL (A|E|I|D|W)
    FUNCTION_NAME ([^:]*)
  '''
  ## multiline parser/codec
  ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html
  [inputs.tail.multiline]
    ## The pattern should be a regexp which matches what you believe to be an indicator that the field is part of an event consisting of multiple lines of log data.
    pattern = '([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2},[0-9]{3})'
    invert_match = true

[[inputs.tail]]
   ## files to tail.
  files = ["/var/log/api/server.log"]
  ## Override the default measurement name, which would be "logparser_grok"
  name_override = "api_log"
  data_format = "grok"
  ## For parsing logstash-style "grok" patterns:
  grok_patterns = ['%{TIMESTAMP_ISO8601:timestamp:ts-"2006-01-02 15:04:05.000"} \<%{LOGLEVEL:log_level:tag}\> %{FUNCTION_NAME:function_name:tag}:%{POSINT:line_number:int}: %{GREEDYDATA:GREEDYDATA}']
  grok_custom_patterns = '''
    LOGLEVEL (A|E|I|D|W)
    FUNCTION_NAME ([^:]*)
  '''
  ## multiline parser/codec
  ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html
  [inputs.tail.multiline]
    ## The pattern should be a regexp which matches what you believe to be an indicator that the field is part of an event consisting of multiple lines of log data.
    pattern = '([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2},[0-9]{3})'
    invert_match = true

[[inputs.tail]]
   ## files to tail.
  files = ["/var/log/monitoring/monitoring.log"]
  ## Override the default measurement name, which would be "logparser_grok"
  name_override = "monitoring_log"
  data_format = "grok"
  ## For parsing logstash-style "grok" patterns:
  grok_patterns = ['%{TIMESTAMP_ISO8601:timestamp:ts-"2006-01-02 15:04:05.000"} \<%{LOGLEVEL:log_level:tag}\> %{FUNCTION_NAME:function_name:tag}:%{POSINT:line_number:int}: %{GREEDYDATA:GREEDYDATA}']
  grok_custom_patterns = '''
    LOGLEVEL (A|E|I|D|W)
    FUNCTION_NAME ([^:]*)
  '''
  ## multiline parser/codec
  ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html
  [inputs.tail.multiline]
    ## The pattern should be a regexp which matches what you believe to be an indicator that the field is part of an event consisting of multiple lines of log data.
    pattern = '([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2},[0-9]{3})'
    invert_match = true

[[inputs.tail]]
   ## files to tail.
  files = ["/var/log/weather/weather.log"]
  ## Override the default measurement name, which would be "logparser_grok"
  name_override = "weather_log"
  data_format = "grok"
  ## For parsing logstash-style "grok" patterns:
  grok_patterns = ['%{TIMESTAMP_ISO8601:timestamp:ts-"2006-01-02 15:04:05.000"} \<%{LOGLEVEL:log_level:tag}\> %{FUNCTION_NAME:function_name:tag}:%{POSINT:line_number:int}: %{GREEDYDATA:GREEDYDATA}']
  grok_custom_patterns = '''
    LOGLEVEL (A|E|I|D|W)
    FUNCTION_NAME ([^:]*)
  '''
  ## multiline parser/codec
  ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html
  [inputs.tail.multiline]
    ## The pattern should be a regexp which matches what you believe to be an indicator that the field is part of an event consisting of multiple lines of log data.
    pattern = '([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2},[0-9]{3})'
    invert_match = true

[[inputs.tail]]
   ## files to tail.
  files = ["/var/log/mosquitto/mosquitto.log"]
  ## Override the default measurement name, which would be "logparser_grok"
  name_override = "mosquitto_log"
  data_format = "grok"
  ## For parsing logstash-style "grok" patterns:
  grok_patterns = ['%{NUMBER:timestamp:ts-epoch}: %{GREEDYDATA:message}']

[[inputs.tail]]
   ## files to tail.
  files = ["/var/log/home/main.log"]
  ## Override the default measurement name, which would be "logparser_grok"
  name_override = "home_log"
  data_format = "grok"
  ## For parsing logstash-style "grok" patterns:
  grok_patterns = ['%{TIMESTAMP:timestamp:ts-"2006/01/02 15:04:05"} %{FUNCTION_NAME:function_name:tag}:%{POSINT:line_number:int}: %{GREEDYDATA:GREEDYDATA}']
  grok_custom_patterns = '''
    TIMESTAMP (%{YEAR}/%{MONTHNUM}/%{MONTHDAY} %{TIME})
    FUNCTION_NAME ([^:]*)
  '''